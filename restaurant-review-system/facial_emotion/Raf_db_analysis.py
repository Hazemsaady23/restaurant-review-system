#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# ============================================================================
# SETUP AND IMPORTS
# ============================================================================

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from PIL import Image

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, WeightedRandomSampler
from torchvision.datasets import ImageFolder
import torchvision.transforms as T
import timm

from sklearn.metrics import (
    confusion_matrix, classification_report, 
    accuracy_score, precision_score, recall_score, f1_score,
    roc_curve, auc, roc_auc_score
)
from sklearn.preprocessing import label_binarize
from sklearn.model_selection import StratifiedKFold

import warnings
warnings.filterwarnings('ignore')


# ============================================================================
# CONFIGURATION
# ============================================================================

ROOT_DIR = r"C:\Users\LENOVO\Desktop\RAF-DB\DATASET"
MODEL_PATH = r"C:\Users\LENOVO\Desktop\Complete_Model\cnn_model\rafdb_efficientnetv2s_best.pth"
OUTPUT_DIR = "./analysis_results"
IMG_SIZE = 224
BATCH_SIZE = 32
EMOTION_LABELS = ['Surprise', 'Fear', 'Disgust', 'Happiness', 'Sadness', 'Anger', 'Neutral']

os.makedirs(OUTPUT_DIR, exist_ok=True)


# ============================================================================
# STEP 5: DATA ANALYSIS AND VISUALIZATION
# ============================================================================

def analyze_dataset_distribution(root_dir):
    print("\n" + "="*80)
    print("STEP 5: DATASET ANALYSIS AND VISUALIZATION")
    print("="*80)
    
    train_dir = os.path.join(root_dir, "train")
    test_dir = os.path.join(root_dir, "test")
    
    train_dataset = ImageFolder(train_dir)
    test_dataset = ImageFolder(test_dir)
    
    train_labels = [label for _, label in train_dataset.samples]
    test_labels = [label for _, label in test_dataset.samples]
    
    all_labels = train_labels + test_labels
    
    class_names = train_dataset.classes
    
    fig = plt.figure(figsize=(20, 12))
    
    # Visualization 1: Train Set Class Distribution
    ax1 = plt.subplot(2, 3, 1)
    train_counts = Counter(train_labels)
    train_sorted = sorted(train_counts.items())
    classes_names = [class_names[i] for i, _ in train_sorted]
    counts = [count for _, count in train_sorted]
    
    bars = ax1.bar(classes_names, counts, color='skyblue', edgecolor='navy', alpha=0.7)
    ax1.set_title('Train Set: Class Distribution', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Emotion', fontsize=12)
    ax1.set_ylabel('Count', fontsize=12)
    ax1.tick_params(axis='x', rotation=45)
    
    for bar, count in zip(bars, counts):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{count}\n({count/sum(counts)*100:.1f}%)',
                ha='center', va='bottom', fontsize=9)
    
    # Visualization 2: Test Set Class Distribution
    ax2 = plt.subplot(2, 3, 2)
    test_counts = Counter(test_labels)
    test_sorted = sorted(test_counts.items())
    test_classes = [class_names[i] for i, _ in test_sorted]
    test_count_vals = [count for _, count in test_sorted]
    
    bars = ax2.bar(test_classes, test_count_vals, color='lightcoral', edgecolor='darkred', alpha=0.7)
    ax2.set_title('Test Set: Class Distribution', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Emotion', fontsize=12)
    ax2.set_ylabel('Count', fontsize=12)
    ax2.tick_params(axis='x', rotation=45)
    
    for bar, count in zip(bars, test_count_vals):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{count}\n({count/sum(test_count_vals)*100:.1f}%)',
                ha='center', va='bottom', fontsize=9)
    
    # Visualization 3: Combined Distribution Comparison
    ax3 = plt.subplot(2, 3, 3)
    x = np.arange(len(class_names))
    width = 0.35
    
    train_vals = [train_counts.get(i, 0) for i in range(len(class_names))]
    test_vals = [test_counts.get(i, 0) for i in range(len(class_names))]
    
    ax3.bar(x - width/2, train_vals, width, label='Train', color='skyblue', alpha=0.8)
    ax3.bar(x + width/2, test_vals, width, label='Test', color='lightcoral', alpha=0.8)
    ax3.set_title('Train vs Test Distribution', fontsize=14, fontweight='bold')
    ax3.set_xlabel('Emotion', fontsize=12)
    ax3.set_ylabel('Count', fontsize=12)
    ax3.set_xticks(x)
    ax3.set_xticklabels(class_names, rotation=45)
    ax3.legend()
    
    # Visualization 4: Class Distribution Pie Chart
    ax4 = plt.subplot(2, 3, 4)
    all_counts = Counter(all_labels)
    all_sorted = sorted(all_counts.items())
    pie_labels = [f"{class_names[i]}\n{count} ({count/sum(all_labels)*100:.1f}%)" 
                  for i, count in all_sorted]
    pie_counts = [count for _, count in all_sorted]
    
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A', '#98D8C8', '#F7DC6F', '#BB8FCE']
    ax4.pie(pie_counts, labels=pie_labels, colors=colors, autopct='', startangle=90)
    ax4.set_title('Overall Class Distribution', fontsize=14, fontweight='bold')
    
    # Visualization 5: Class Balance Analysis
    ax5 = plt.subplot(2, 3, 5)
    percentages = [count/sum(counts)*100 for count in counts]
    colors_balance = ['green' if p > 10 else 'orange' if p > 5 else 'red' for p in percentages]
    
    bars = ax5.barh(classes_names, percentages, color=colors_balance, alpha=0.7)
    ax5.set_title('Class Balance Analysis (Train Set)', fontsize=14, fontweight='bold')
    ax5.set_xlabel('Percentage (%)', fontsize=12)
    ax5.axvline(x=100/len(class_names), color='black', linestyle='--', linewidth=2, label='Perfect Balance')
    ax5.legend()
    
    for i, (bar, pct) in enumerate(zip(bars, percentages)):
        ax5.text(pct + 0.5, bar.get_y() + bar.get_height()/2,
                f'{pct:.1f}%', va='center', fontsize=9)
    
    # Visualization 6: Imbalance Ratio Heatmap
    ax6 = plt.subplot(2, 3, 6)
    max_count = max(counts)
    min_count = min(counts)
    imbalance_ratio = max_count / min_count
    
    imbalance_data = np.array([[count/max_count for count in counts]])
    
    sns.heatmap(imbalance_data, annot=True, fmt='.2f', cmap='RdYlGn', 
                xticklabels=classes_names, yticklabels=['Ratio'], 
                cbar_kws={'label': 'Balance Ratio'}, ax=ax6, vmin=0, vmax=1)
    ax6.set_title(f'Class Balance Heatmap\nImbalance Ratio: {imbalance_ratio:.2f}:1', 
                  fontsize=14, fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, '1_dataset_distribution.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    print("\nüìä Dataset Statistics:")
    print(f"   Total Train Samples: {len(train_labels)}")
    print(f"   Total Test Samples: {len(test_labels)}")
    print(f"   Number of Classes: {len(class_names)}")
    print(f"   Imbalance Ratio: {imbalance_ratio:.2f}:1")
    
    return train_dataset, test_dataset, train_labels, test_labels, class_names


# ============================================================================
# STEP 6: CHECK AND HANDLE IMBALANCE
# ============================================================================

def check_and_visualize_imbalance(train_labels, class_names):
    print("\n" + "="*80)
    print("STEP 6: IMBALANCE ANALYSIS AND HANDLING")
    print("="*80)
    
    train_counts = Counter(train_labels)
    total = len(train_labels)
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # Before handling imbalance
    sorted_counts = sorted(train_counts.items())
    classes = [class_names[i] for i, _ in sorted_counts]
    counts = [count for _, count in sorted_counts]
    percentages = [count/total*100 for count in counts]
    
    # Visualization 7: Imbalance Detection
    ax1 = axes[0, 0]
    colors = ['red' if p < 10 else 'orange' if p < 15 else 'green' for p in percentages]
    bars = ax1.bar(classes, percentages, color=colors, alpha=0.7, edgecolor='black')
    ax1.axhline(y=100/len(class_names), color='blue', linestyle='--', linewidth=2, label='Perfect Balance')
    ax1.set_title('Class Imbalance Detection (Before)', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Emotion', fontsize=12)
    ax1.set_ylabel('Percentage (%)', fontsize=12)
    ax1.tick_params(axis='x', rotation=45)
    ax1.legend()
    
    for bar, pct in zip(bars, percentages):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height,
                f'{pct:.1f}%', ha='center', va='bottom', fontsize=9)
    
    # Check imbalance severity
    max_pct = max(percentages)
    min_pct = min(percentages)
    is_imbalanced = max_pct / min_pct > 1.5
    
    # Visualization 8: Class Weights
    ax2 = axes[0, 1]
    class_weights = 1.0 / (np.array(counts) + 1e-6)
    class_weights = class_weights * (len(class_names) / class_weights.sum())
    
    bars = ax2.bar(classes, class_weights, color='purple', alpha=0.7, edgecolor='black')
    ax2.set_title('Computed Class Weights', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Emotion', fontsize=12)
    ax2.set_ylabel('Weight', fontsize=12)
    ax2.tick_params(axis='x', rotation=45)
    
    for bar, weight in zip(bars, class_weights):
        height = bar.get_height()
        ax2.text(bar.get_x() + bar.get_width()/2., height,
                f'{weight:.2f}', ha='center', va='bottom', fontsize=9)
    
    # Visualization: After weighted sampling (simulated)
    ax3 = axes[1, 0]
    target_count = max(counts)
    balanced_counts = [target_count] * len(classes)
    balanced_pct = [count/sum(balanced_counts)*100 for count in balanced_counts]
    
    bars = ax3.bar(classes, balanced_pct, color='green', alpha=0.7, edgecolor='black')
    ax3.axhline(y=100/len(class_names), color='blue', linestyle='--', linewidth=2, label='Perfect Balance')
    ax3.set_title('Expected Distribution (After Weighting)', fontsize=14, fontweight='bold')
    ax3.set_xlabel('Emotion', fontsize=12)
    ax3.set_ylabel('Percentage (%)', fontsize=12)
    ax3.tick_params(axis='x', rotation=45)
    ax3.legend()
    
    for bar, pct in zip(bars, balanced_pct):
        height = bar.get_height()
        ax3.text(bar.get_x() + bar.get_width()/2., height,
                f'{pct:.1f}%', ha='center', va='bottom', fontsize=9)
    
    # Comparison table
    ax4 = axes[1, 1]
    ax4.axis('tight')
    ax4.axis('off')
    
    table_data = []
    for i, cls in enumerate(classes):
        table_data.append([
            cls,
            counts[i],
            f'{percentages[i]:.1f}%',
            f'{class_weights[i]:.2f}',
            'Imbalanced' if percentages[i] < 10 else 'Balanced'
        ])
    
    table = ax4.table(cellText=table_data,
                     colLabels=['Class', 'Count', 'Percentage', 'Weight', 'Status'],
                     cellLoc='center',
                     loc='center',
                     bbox=[0, 0, 1, 1])
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2)
    
    for i in range(len(table_data) + 1):
        if i == 0:
            table[(i, 0)].set_facecolor('#40466e')
            table[(i, 0)].set_text_props(weight='bold', color='white')
        else:
            if 'Imbalanced' in table_data[i-1]:
                for j in range(5):
                    table[(i, j)].set_facecolor('#ffcccc')
    
    ax4.set_title('Imbalance Summary', fontsize=14, fontweight='bold', pad=20)
    
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, '2_imbalance_analysis.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    print(f"\nüìä Imbalance Analysis:")
    print(f"   Is Imbalanced: {'Yes' if is_imbalanced else 'No'}")
    print(f"   Max/Min Ratio: {max_pct/min_pct:.2f}:1")
    print(f"   Recommendation: {'Use class weights' if is_imbalanced else 'No action needed'}")
    
    return class_weights, is_imbalanced


# ============================================================================
# STEP 7: MODEL LOADING AND EVALUATION
# ============================================================================

def load_model(model_path, num_classes=7):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
    model = timm.create_model(
        "tf_efficientnetv2_s_in21k",
        pretrained=False,
        num_classes=num_classes,
        drop_rate=0.4,
        drop_path_rate=0.2,
    )
    
    model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))
    model.to(device)
    model.eval()
    
    return model, device


def get_predictions(model, dataset, device, batch_size=32):
    transform = T.Compose([
        T.Resize((IMG_SIZE, IMG_SIZE)),
        T.CenterCrop(IMG_SIZE),
        T.ToTensor(),
        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ])
    
    dataset.transform = transform
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=4)
    
    all_preds = []
    all_labels = []
    all_probs = []
    
    with torch.no_grad():
        for images, labels in loader:
            images = images.to(device)
            outputs = model(images)
            probs = F.softmax(outputs, dim=1)
            _, preds = outputs.max(1)
            
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.numpy())
            all_probs.extend(probs.cpu().numpy())
    
    return np.array(all_preds), np.array(all_labels), np.array(all_probs)


# ============================================================================
# STEP 8: EVALUATE MODEL PERFORMANCE
# ============================================================================

def evaluate_model_performance(model, test_dataset, device, class_names):
    print("\n" + "="*80)
    print("STEP 8: MODEL PERFORMANCE EVALUATION")
    print("="*80)
    
    preds, labels, probs = get_predictions(model, test_dataset, device)
    
    # Calculate metrics
    accuracy = accuracy_score(labels, preds)
    precision = precision_score(labels, preds, average='weighted')
    recall = recall_score(labels, preds, average='weighted')
    f1 = f1_score(labels, preds, average='weighted')
    
    print(f"\nüìä Overall Metrics:")
    print(f"   Accuracy:  {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"   Precision: {precision:.4f}")
    print(f"   Recall:    {recall:.4f}")
    print(f"   F1-Score:  {f1:.4f}")
    
    # Per-class metrics
    print(f"\nüìä Per-Class Metrics:")
    class_report = classification_report(labels, preds, target_names=class_names, digits=4)
    print(class_report)
    
    # Create visualization figure
    fig = plt.figure(figsize=(20, 12))
    
    # Visualization 9: Confusion Matrix
    ax1 = plt.subplot(2, 3, 1)
    cm = confusion_matrix(labels, preds)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names,
                yticklabels=class_names, ax=ax1, cbar_kws={'label': 'Count'})
    ax1.set_title('Confusion Matrix', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Predicted', fontsize=12)
    ax1.set_ylabel('True', fontsize=12)
    
    # Visualization 10: Normalized Confusion Matrix
    ax2 = plt.subplot(2, 3, 2)
    cm_norm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    sns.heatmap(cm_norm, annot=True, fmt='.2f', cmap='RdYlGn', xticklabels=class_names,
                yticklabels=class_names, ax=ax2, vmin=0, vmax=1, cbar_kws={'label': 'Proportion'})
    ax2.set_title('Normalized Confusion Matrix', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Predicted', fontsize=12)
    ax2.set_ylabel('True', fontsize=12)
    
    # Visualization 11: Per-Class Performance
    ax3 = plt.subplot(2, 3, 3)
    report_dict = classification_report(labels, preds, target_names=class_names, output_dict=True)
    
    metrics_data = []
    for cls in class_names:
        metrics_data.append([
            report_dict[cls]['precision'],
            report_dict[cls]['recall'],
            report_dict[cls]['f1-score']
        ])
    
    x = np.arange(len(class_names))
    width = 0.25
    
    ax3.bar(x - width, [m[0] for m in metrics_data], width, label='Precision', alpha=0.8)
    ax3.bar(x, [m[1] for m in metrics_data], width, label='Recall', alpha=0.8)
    ax3.bar(x + width, [m[2] for m in metrics_data], width, label='F1-Score', alpha=0.8)
    
    ax3.set_title('Per-Class Performance Metrics', fontsize=14, fontweight='bold')
    ax3.set_xlabel('Emotion', fontsize=12)
    ax3.set_ylabel('Score', fontsize=12)
    ax3.set_xticks(x)
    ax3.set_xticklabels(class_names, rotation=45)
    ax3.legend()
    ax3.set_ylim([0, 1.1])
    
    # Visualization 12: ROC Curves
    ax4 = plt.subplot(2, 3, 4)
    
    labels_bin = label_binarize(labels, classes=range(len(class_names)))
    
    for i in range(len(class_names)):
        fpr, tpr, _ = roc_curve(labels_bin[:, i], probs[:, i])
        roc_auc = auc(fpr, tpr)
        ax4.plot(fpr, tpr, label=f'{class_names[i]} (AUC = {roc_auc:.2f})', linewidth=2)
    
    ax4.plot([0, 1], [0, 1], 'k--', linewidth=2, label='Random')
    ax4.set_title('ROC Curves (One-vs-Rest)', fontsize=14, fontweight='bold')
    ax4.set_xlabel('False Positive Rate', fontsize=12)
    ax4.set_ylabel('True Positive Rate', fontsize=12)
    ax4.legend(loc='lower right', fontsize=8)
    ax4.grid(alpha=0.3)
    
    # Visualization 13: Metrics Comparison
    ax5 = plt.subplot(2, 3, 5)
    metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
    metrics_values = [accuracy, precision, recall, f1]
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A']
    
    bars = ax5.barh(metrics_names, metrics_values, color=colors, alpha=0.7, edgecolor='black')
    ax5.set_title('Overall Performance Metrics', fontsize=14, fontweight='bold')
    ax5.set_xlabel('Score', fontsize=12)
    ax5.set_xlim([0, 1])
    
    for bar, val in zip(bars, metrics_values):
        ax5.text(val + 0.02, bar.get_y() + bar.get_height()/2,
                f'{val:.4f}', va='center', fontsize=11, fontweight='bold')
    
    # Visualization: Class-wise Accuracy
    ax6 = plt.subplot(2, 3, 6)
    class_acc = []
    for i in range(len(class_names)):
        mask = labels == i
        if mask.sum() > 0:
            class_acc.append((preds[mask] == labels[mask]).mean())
        else:
            class_acc.append(0)
    
    bars = ax6.bar(class_names, class_acc, color='steelblue', alpha=0.7, edgecolor='black')
    ax6.axhline(y=accuracy, color='red', linestyle='--', linewidth=2, label=f'Overall Acc: {accuracy:.3f}')
    ax6.set_title('Per-Class Accuracy', fontsize=14, fontweight='bold')
    ax6.set_xlabel('Emotion', fontsize=12)
    ax6.set_ylabel('Accuracy', fontsize=12)
    ax6.tick_params(axis='x', rotation=45)
    ax6.legend()
    ax6.set_ylim([0, 1.1])
    
    for bar, acc in zip(bars, class_acc):
        height = bar.get_height()
        ax6.text(bar.get_x() + bar.get_width()/2., height,
                f'{acc:.3f}', ha='center', va='bottom', fontsize=9)
    
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, '3_model_performance.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    return accuracy, precision, recall, f1


# ============================================================================
# STEP 7: LEARNING CURVES AND OVERFITTING ANALYSIS
# ============================================================================

def create_learning_curves_analysis():
    print("\n" + "="*80)
    print("STEP 7: OVERFITTING/UNDERFITTING ANALYSIS")
    print("="*80)
    
    fig, axes = plt.subplots(2, 2, figsize=(16, 12))
    
    # Simulated learning curves (you should replace with actual training history)
    epochs = np.arange(1, 51)
    train_loss = 2.0 * np.exp(-epochs/10) + 0.1 + np.random.normal(0, 0.05, 50)
    val_loss = 2.0 * np.exp(-epochs/10) + 0.15 + np.random.normal(0, 0.08, 50)
    train_acc = 1 - np.exp(-epochs/8) * 0.9 + np.random.normal(0, 0.02, 50)
    val_acc = 1 - np.exp(-epochs/8) * 0.9 - 0.05 + np.random.normal(0, 0.03, 50)
    
    # Visualization: Loss Curves
    ax1 = axes[0, 0]
    ax1.plot(epochs, train_loss, label='Train Loss', linewidth=2, color='blue')
    ax1.plot(epochs, val_loss, label='Validation Loss', linewidth=2, color='red')
    ax1.fill_between(epochs, train_loss, val_loss, alpha=0.2, color='gray')
    ax1.set_title('Learning Curves: Loss', fontsize=14, fontweight='bold')
    ax1.set_xlabel('Epoch', fontsize=12)
    ax1.set_ylabel('Loss', fontsize=12)
    ax1.legend()
    ax1.grid(alpha=0.3)
    
    # Visualization: Accuracy Curves
    ax2 = axes[0, 1]
    ax2.plot(epochs, train_acc, label='Train Accuracy', linewidth=2, color='blue')
    ax2.plot(epochs, val_acc, label='Validation Accuracy', linewidth=2, color='red')
    ax2.fill_between(epochs, train_acc, val_acc, alpha=0.2, color='gray')
    ax2.set_title('Learning Curves: Accuracy', fontsize=14, fontweight='bold')
    ax2.set_xlabel('Epoch', fontsize=12)
    ax2.set_ylabel('Accuracy', fontsize=12)
    ax2.legend()
    ax2.grid(alpha=0.3)
    
    # Visualization: Overfitting Detection
    ax3 = axes[1, 0]
    gap = np.abs(train_loss - val_loss)
    ax3.plot(epochs, gap, linewidth=2, color='orange')
    ax3.fill_between(epochs, 0, gap, alpha=0.3, color='orange')
    ax3.axhline(y=0.1, color='red', linestyle='--', linewidth=2, label='Overfitting Threshold')
    ax3.set_title('Train-Validation Gap (Overfitting Indicator)', fontsize=14, fontweight='bold')
    ax3.set_xlabel('Epoch', fontsize=12)
    ax3.set_ylabel('Loss Gap', fontsize=12)
    ax3.legend()
    ax3.grid(alpha=0.3)
    
    # Visualization: Regularization Effects
    ax4 = axes[1, 1]
    ax4.axis('tight')
    ax4.axis('off')
    
    regularization_info = [
        ['Technique', 'Applied', 'Effect'],
        ['Dropout (0.4)', 'Yes', 'Reduces overfitting'],
        ['DropPath (0.2)', 'Yes', 'Improves generalization'],
        ['Label Smoothing (0.1)', 'Yes', 'Prevents overconfidence'],
        ['Data Augmentation', 'Yes', 'Increases data diversity'],
        ['Class Weights', 'Yes', 'Handles imbalance'],
        ['Early Stopping', 'Recommended', 'Stops at optimal point']
    ]
    
    table = ax4.table(cellText=regularization_info, cellLoc='left', loc='center',
                     bbox=[0, 0, 1, 1])
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2.5)
    
    for i in range(len(regularization_info)):
        if i == 0:
            for j in range(3):
                table[(i, j)].set_facecolor('#40466e')
                table[(i, j)].set_text_props(weight='bold', color='white')
        else:
            if 'Yes' in regularization_info[i][1]:
                table[(i, 1)].set_facecolor('#90EE90')
    
    ax4.set_title('Regularization Techniques Applied', fontsize=14, fontweight='bold', pad=20)
    
    plt.tight_layout()
    plt.savefig(os.path.join(OUTPUT_DIR, '4_overfitting_analysis.png'), dpi=300, bbox_inches='tight')
    plt.close()
    
    print("\nüìä Overfitting Analysis:")
    print("   - Dropout rate: 0.4")
    print("   - DropPath rate: 0.2")
    print("   - Label smoothing: 0.1")
    print("   - Data augmentation: Applied")
    print("   - Recommendation: Monitor validation loss for early stopping")


# ============================================================================
# MAIN ANALYSIS PIPELINE
# ============================================================================

def main():
    print("\n" + "="*80)
    print("RAF-DB EMOTION RECOGNITION: COMPREHENSIVE ANALYSIS")
    print("="*80)
    
    # Step 5: Dataset Analysis
    train_dataset, test_dataset, train_labels, test_labels, class_names = \
        analyze_dataset_distribution(ROOT_DIR)
    
    # Step 6: Imbalance Analysis
    class_weights, is_imbalanced = check_and_visualize_imbalance(train_labels, class_names)
    
    # Step 7: Overfitting Analysis
    create_learning_curves_analysis()
    
    # Step 8: Model Performance Evaluation
    if os.path.exists(MODEL_PATH):
        print(f"\nüì¶ Loading model from: {MODEL_PATH}")
        model, device = load_model(MODEL_PATH, num_classes=len(class_names))
        accuracy, precision, recall, f1 = evaluate_model_performance(
            model, test_dataset, device, class_names
        )
        
        # Summary Report
        print("\n" + "="*80)
        print("FINAL SUMMARY REPORT")
        print("="*80)
        print(f"\n‚úÖ Dataset Information:")
        print(f"   - Total Classes: {len(class_names)}")
        print(f"   - Training Samples: {len(train_labels)}")
        print(f"   - Test Samples: {len(test_labels)}")
        print(f"   - Imbalanced: {'Yes' if is_imbalanced else 'No'}")
        
        print(f"\n‚úÖ Model Performance:")
        print(f"   - Accuracy:  {accuracy:.4f}")
        print(f"   - Precision: {precision:.4f}")
        print(f"   - Recall:    {recall:.4f}")
        print(f"   - F1-Score:  {f1:.4f}")
        
        print(f"\n‚úÖ Visualizations saved to: {OUTPUT_DIR}")
        print("   1. 1_dataset_distribution.png")
        print("   2. 2_imbalance_analysis.png")
        print("   3. 3_model_performance.png")
        print("   4. 4_overfitting_analysis.png")
    else:
        print(f"\n‚ùå Model not found at: {MODEL_PATH}")
        print("   Please train the model first using rafdb_training.py")
    
    print("\n" + "="*80)


# ============================================================================
# ENTRY POINT
# ============================================================================

if __name__ == "__main__":
    main()